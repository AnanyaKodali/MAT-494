{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOod8yfodOtJoSosSlacftg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnanyaKodali/MAT-494/blob/main/3_7_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3.7: Neural Networks**"
      ],
      "metadata": {
        "id": "FUozqcvPZd7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.1**\n",
        "\n",
        "Neural Networks are computational models inspired by the human brain's structure and functionality. They consist of layers of interconnected nodes, known as neurons, which process and transmit information. Each neuron receives input, applies a transformation through an activation function, and passes the output to subsequent layers. Neural networks are designed to recognize patterns, learn from data, and make intelligent decisions or predictions."
      ],
      "metadata": {
        "id": "0k7VU439Zno0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.2**\n",
        "\n",
        "Structure of Neural Networks:\n",
        "* Input Layer: Receives the initial data.\n",
        "* Hidden Layers: Intermediate layers that transform the input using weights, biases, and activation functions.\n",
        "* Output Layer: Produces the final prediction or classification.\n"
      ],
      "metadata": {
        "id": "fRNeKJVqZvJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.7.1: Mathematical Formulation**"
      ],
      "metadata": {
        "id": "-oWatUmmbJn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.3**\n",
        "\n",
        "In a neural network, each neuron performs a weighted sum of its inputs, adds a bias, and applies an activation function to produce its output. This process can be mathematically described as follows for layer $ùëô$:\n",
        "* $\\begin{aligned}\n",
        "\\mathbf{Z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{A}^{(l-1)} + \\mathbf{b}^{(l)} \\\\\n",
        "\\mathbf{A}^{(l)}= \\sigma(\\mathbf{Z}^{(l)})\n",
        "\\end{aligned}$\n",
        "\n",
        "Where,\n",
        "* $\\mathbf{W}^{(l)} \\in \\mathbb{R}^{n^{(l)} \\times n^{(l-1)}}$ is the weight matrix for layer $ùëô$\n",
        "* $\\mathbf{b}^{(l)} \\in \\mathbb{R}^{n^{(l)} \\times 1}$ is the bias vector for layer $ùëô$\n",
        "* $\\mathbf{A}^{(l-1)} \\in \\mathbb{R}^{n^{(l-1)} \\times m}$  is the activation matrix from the previous layer\n",
        "* $\\mathbf{Z}^{(l)} \\in \\mathbb{R}^{n^{(l)} \\times m}$ is the linear combination of inputs and weights\n",
        "* $\\mathbf{A}^{(l)} \\in \\mathbb{R}^{n^{(l)} \\times m}$  is the activation output.\n",
        "* $œÉ$ is the activation function.\n",
        "* $ùëö$ is the number of training examples."
      ],
      "metadata": {
        "id": "3ymPIESdZ4h-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3.7.1: Simple Neural Network**\n",
        "\n",
        "Objective: Initialization of parameters and forward propagation for a simple neural network"
      ],
      "metadata": {
        "id": "wwDtjq8ta2FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Networks: Initialization and Forward Propagation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Initialize parameters\n",
        "def initialize_parameters(layer_dims):\n",
        "    \"\"\"\n",
        "    Initialize weights and biases for each layer.\n",
        "\n",
        "    Arguments:\n",
        "    layer_dims -- List containing the dimensions of each layer.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- Dictionary containing initialized weights and biases.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)  # Number of layers\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the neural network.\n",
        "\n",
        "    Arguments:\n",
        "    X -- Input data of shape (n_x, m).\n",
        "    parameters -- Dictionary containing weights and biases.\n",
        "\n",
        "    Returns:\n",
        "    activations -- Dictionary containing activations for each layer.\n",
        "    \"\"\"\n",
        "    activations = {\"A0\": X}\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "\n",
        "    for l in range(1, L + 1):\n",
        "        W = parameters[f\"W{l}\"]\n",
        "        b = parameters[f\"b{l}\"]\n",
        "        Z = np.dot(W, activations[f\"A{l-1}\"]) + b\n",
        "        A = relu(Z)  # Using ReLU activation\n",
        "        activations[f\"A{l}\"] = A\n",
        "\n",
        "    return activations\n",
        "\n",
        "# ReLU Activation Function\n",
        "def relu(x):\n",
        "    \"\"\"ReLU activation function.\"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define layer dimensions\n",
        "    layer_dims = [2, 4, 1]  # 2-input features, 4 neurons in hidden layer, 1 output neuron\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(layer_dims)\n",
        "\n",
        "    # Generate dummy input data (2 features, 3 examples)\n",
        "    X = np.array([[1, 2, -1],\n",
        "                  [3, -1, 2]])\n",
        "\n",
        "    # Perform forward propagation\n",
        "    activations = forward_propagation(X, parameters)\n",
        "\n",
        "    # Print activations\n",
        "    for key in activations:\n",
        "        print(f\"{key}:\\n{activations[key]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRmJofkXbCxb",
        "outputId": "5364f3c6-85e3-47ad-982a-6210fa0a699f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A0:\n",
            "[[ 1  2 -1]\n",
            " [ 3 -1  2]]\n",
            "\n",
            "A1:\n",
            "[[0.00081921 0.01131693 0.        ]\n",
            " [0.05216778 0.         0.02398371]\n",
            " [0.         0.         0.        ]\n",
            " [0.03881517 0.02390991 0.        ]]\n",
            "\n",
            "A2:\n",
            "[[9.84217472e-05 0.00000000e+00 1.30126037e-04]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.7.2: Activation Functions**"
      ],
      "metadata": {
        "id": "gxN5HYCcbiQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.4**\n",
        "\n",
        "Activation functions introduce non-linearity into the neural network, enabling it to learn complex patterns. Below are four common activation functions used in neural networks.\n",
        "\n"
      ],
      "metadata": {
        "id": "HACgdDQRbqDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.4**\n",
        "\n",
        "*Step Function*\n",
        "\n",
        "The Step Function activates the neuron if the input exceeds a certain threshold. It's a binary function returning 1 or 0.\n",
        "* $\\sigma(x) =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } x \\geq 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}$"
      ],
      "metadata": {
        "id": "ZAQ3Zd-AbzMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step Function Activation\n",
        "\n",
        "def binary_step(x):\n",
        "    \"\"\"\n",
        "    Binary Step Activation Function.\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input array.\n",
        "\n",
        "    Returns:\n",
        "    Output array after applying binary step function.\n",
        "    \"\"\"\n",
        "    return np.heaviside(x, 1)"
      ],
      "metadata": {
        "id": "-5j3F_OFcNZT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Binary Step Function\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "print(\"Binary Step Output:\", binary_step(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA-kRPHicUJJ",
        "outputId": "3ef8e96d-88d1-4218-8037-526866714e36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Step Output: [0. 0. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU Function"
      ],
      "metadata": {
        "id": "xW9A4qdcbkw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.5**\n",
        "\n",
        "The Rectified Linear Unit (ReLU) activation function outputs the input directly if it is positive; otherwise, it outputs zero\n",
        "* $\\sigma(x) = \\max(0, x)$"
      ],
      "metadata": {
        "id": "bP418IYucgxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU Activation Function\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    ReLU Activation Function.\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input array.\n",
        "\n",
        "    Returns:\n",
        "    Output array after applying ReLU function.\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "bshyW3jDcuL3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60nQOa3tYQIC",
        "outputId": "95e54198-b9fb-4aab-d06e-83cbb1688eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU Output: [0 0 0 1 2]\n"
          ]
        }
      ],
      "source": [
        "# Example of ReLU Function\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "print(\"ReLU Output:\", relu(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sigmoid Function"
      ],
      "metadata": {
        "id": "GswwFB1fbk2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.6**\n",
        "\n",
        "The Sigmoid activation function maps any real-valued number into the range (0, 1), making it suitable for binary classification.\n",
        "* $\\sigma(x) = \\frac{1}{1 + e^{-x}}$"
      ],
      "metadata": {
        "id": "P9t7xQ57c68t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid Activation Function\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid Activation Function.\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input array.\n",
        "\n",
        "    Returns:\n",
        "    Output array after applying sigmoid function.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "vfmSjrJ4dIIP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Sigmoid Function\n",
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "print(\"Sigmoid Output:\", sigmoid(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGy1sCi2dLdC",
        "outputId": "db2ee4bc-7484-44cc-d2ea-e6e1ad6ad006"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Output: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Softmax Function"
      ],
      "metadata": {
        "id": "jYxL0klUdSlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.7**\n",
        "\n",
        "The Softmax activation function generalizes the sigmoid function for multiclass classification, converting logits into probabilities that sum to 1.\n",
        "\n",
        "* $\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}$"
      ],
      "metadata": {
        "id": "HtWVzWvtdV6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation Function\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Softmax Activation Function.\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input array of shape (n_classes, m).\n",
        "\n",
        "    Returns:\n",
        "    Output array after applying softmax function.\n",
        "    \"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=0, keepdims=True))  # Stability improvement\n",
        "    return e_x / np.sum(e_x, axis=0, keepdims=True)"
      ],
      "metadata": {
        "id": "auhbIDQ0dg5M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Softmax Function\n",
        "x = np.array([[2.0, 1.0, 0.1],\n",
        "              [1.0, 3.0, 0.2],\n",
        "              [0.2, 0.5, 2.0]])\n",
        "print(\"Softmax Output:\\n\", softmax(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy5an0m4dkq1",
        "outputId": "d25ef304-bd00-488f-ef63-41514de7571c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Output:\n",
            " [[0.65223985 0.11116562 0.11375186]\n",
            " [0.23994563 0.82140902 0.12571524]\n",
            " [0.10781452 0.06742536 0.7605329 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.7.3. Cost Function**"
      ],
      "metadata": {
        "id": "d8T0yBowc134"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cross-Entropy Loss"
      ],
      "metadata": {
        "id": "SbSEEu58dzeo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.8**\n",
        "\n",
        "The Cross-Entropy Loss is commonly used for binary classification tasks. It measures the difference between the true labels and the predicted probabilities.\n",
        "* $J = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]$\n",
        "\n",
        "Where,\n",
        "* $m$ is the number of training examples\n",
        "* $y^{(i)}$ is the true label (0 or 1) for example $i$\n",
        "* $\\hat{y}^{(i)}$ is the predicted probability for example $i$"
      ],
      "metadata": {
        "id": "jofDn-1Ed_hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-Entropy Loss Function\n",
        "\n",
        "def cost_function(AL, Y):\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy loss.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- Probability vector corresponding to label predictions, shape (1, m)\n",
        "    Y -- True \"label\" vector, shape (1, m)\n",
        "\n",
        "    Returns:\n",
        "    cost -- Cross-entropy cost\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "    # To prevent log(0), clip AL to [1e-15, 1 - 1e-15]\n",
        "    AL = np.clip(AL, 1e-15, 1 - 1e-15)\n",
        "    cost = - (1/m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
        "    return np.squeeze(cost)"
      ],
      "metadata": {
        "id": "dJiGrTczehVz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Analysis*\n",
        "1. Input Parameters:\n",
        " * AL: Predicted probabilities from the output layer (shape: (1, m)).\n",
        " * Y: True labels (shape: (1, m)).\n",
        "\n",
        "2. Clipping AL:\n",
        " * Prevents taking the logarithm of 0 by ensuring all probability values are within (1e-15,1-1e-15).\n",
        "3. Cost Calculation:\n",
        " * Computes the average cross-entropy loss over all training examples."
      ],
      "metadata": {
        "id": "DMdWK8igeig0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Cross-Entropy Loss Function\n",
        "AL = sigmoid(np.array([[0.9, 0.2, 0.8, 0.4]]))\n",
        "Y = np.array([[1, 0, 1, 0]])\n",
        "print(\"Cross-Entropy Loss:\", cost_function(AL, Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbi7bw7ze75Y",
        "outputId": "b89cd882-fe5d-4307-db44-38930a37a21b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Entropy Loss: 0.6058521656153525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.7.4: Backpropagation**"
      ],
      "metadata": {
        "id": "9lQ0MGk4fHdo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definition 3.7.9**\n",
        "\n",
        "Backpropagation is the cornerstone of training neural networks. It efficiently computes the gradients of the loss function with respect to each weight and bias by applying the chain rule of calculus. This process allows the network to update its parameters to minimize the loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "tQqVRtHhfKpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mathematical Intuition 3.7.10**\n",
        "\n",
        "For each layer $l$ in the network, backpropagation involves:\n",
        "1. Compute Gradient of Cost with Respect to Output Activation:\n",
        " * $\\delta^{(L)} = \\mathbf{A}^{(L)} - \\mathbf{Y}$\n",
        " * where $L$ is the output layer.\n",
        "2. Compute Gradients for Each Layer:\n",
        " * $\\delta^{(l)} = (\\mathbf{W}^{(l+1)})^T \\delta^{(l+1)} \\cdot g'(Z^{(l)})$\n",
        " * where $ùëî^‚Ä≤$ is the derivative of the activation function.\n",
        "3. Compute Gradients with Respect to Weights and Biases:\n",
        " * $\\frac{\\partial J}{\\partial \\mathbf{W}^{(l)}} = \\frac{1}{m} \\delta^{(l)} \\mathbf{A}^{(l-1)T}$\n",
        " * $\\frac{\\partial J}{\\partial \\mathbf{b}^{(l)}} = \\frac{1}{m} \\sum \\delta^{(l)}$"
      ],
      "metadata": {
        "id": "86_in42FfVBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation Implementation\n",
        "\n",
        "def backward_propagation(parameters, activations, Y):\n",
        "    \"\"\"\n",
        "    Implement backpropagation to compute gradients.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- Dictionary containing weights and biases.\n",
        "    activations -- Dictionary containing activations from forward propagation.\n",
        "    Y -- True labels, shape (1, m)\n",
        "\n",
        "    Returns:\n",
        "    grads -- Dictionary containing gradients with respect to each parameter.\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Initialize backpropagation\n",
        "    # Compute derivative for the output layer\n",
        "    A_final = activations[f\"A{L}\"]\n",
        "    dZ = A_final - Y  # For sigmoid activation and cross-entropy loss\n",
        "    grads[f\"dW{L}\"] = (1/m) * np.dot(dZ, activations[f\"A{L-1}\"].T)\n",
        "    grads[f\"db{L}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "    # Loop from l=L-1 to l=1\n",
        "    for l in reversed(range(1, L)):\n",
        "        W_next = parameters[f\"W{l+1}\"]\n",
        "        dA = np.dot(W_next.T, dZ)\n",
        "        Z = np.dot(parameters[f\"W{l}\"], activations[f\"A{l-1}\"]) + parameters[f\"b{l}\"]\n",
        "        dZ = dA * relu_derivative(Z)\n",
        "        grads[f\"dW{l}\"] = (1/m) * np.dot(dZ, activations[f\"A{l-1}\"].T)\n",
        "        grads[f\"db{l}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "    return grads\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    \"\"\"\n",
        "    Compute the derivative of ReLU activation function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Linear combination input to the activation function.\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of ReLU with respect to Z.\n",
        "    \"\"\"\n",
        "    dZ = np.array(Z > 0, dtype=float)\n",
        "    return dZ"
      ],
      "metadata": {
        "id": "fFnSo7XWgJO9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Analysis*\n",
        "1. Initialization:\n",
        " * grads: Dictionary to store gradients.\n",
        " * L: Number of layers.\n",
        " * m: Number of training examples.\n",
        "\n",
        "2. Output Layer Gradient (dZ):\n",
        " * For sigmoid activation combined with cross-entropy loss, the gradient simplifies to $\\mathbf{A}^{(L)} - \\mathbf{Y}$\n",
        "3. Gradients for Output Layer:\n",
        " * dW and db are computed using the gradients.\n",
        "4. Backpropagation through Hidden Layers:\n",
        " * dA: Gradient of the activation from the next layer.\n",
        " * dZ: Gradient with respect to $Z^{(l)}$, applying the derivative of the activation function (ReLU in this case).\n",
        " * grads[\"dW{l}\"] and grads[\"db{l}\"]: Gradients with respect to weights and biases.\n",
        "5. ReLU Derivative (relu_derivative):\n",
        " * Returns 1 where Z &gt; 0, else 0."
      ],
      "metadata": {
        "id": "7YGxS_D7gQJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Backpropagation\n",
        "\n",
        "# Assume we have already performed forward propagation\n",
        "parameters = {\n",
        "    \"W1\": np.array([[0.1, -0.2],\n",
        "                   [0.4, 0.5],\n",
        "                   [-0.3, 0.2],\n",
        "                   [0.1, -0.5]]),\n",
        "    \"b1\": np.array([[0.0],\n",
        "                   [0.0],\n",
        "                   [0.0],\n",
        "                   [0.0]]),\n",
        "    \"W2\": np.array([[0.3, -0.1, 0.2, 0.4]]),\n",
        "    \"b2\": np.array([[0.0]])\n",
        "}\n",
        "\n",
        "# Dummy activations\n",
        "activations = {\n",
        "    \"A0\": np.array([[1, 2, -1],\n",
        "                   [3, -1, 2]]),\n",
        "    \"A1\": relu(np.dot(parameters[\"W1\"], activations[\"A0\"]) + parameters[\"b1\"]),\n",
        "    \"A2\": sigmoid(np.dot(parameters[\"W2\"], activations[\"A1\"]) + parameters[\"b2\"])\n",
        "}\n",
        "\n",
        "# True labels\n",
        "Y = np.array([[1, 0, 1]])\n",
        "\n",
        "# Perform backpropagation\n",
        "grads = backward_propagation(parameters, activations, Y)\n",
        "\n",
        "# Print gradients\n",
        "for key in grads:\n",
        "    print(f\"{key}:\\n{grads[key]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "709DvEjYhEWr",
        "outputId": "c0af6468-beae-4e7a-da63-31f3a25cf651"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dW2:\n",
            "[[ 0.0788612  -0.37407729 -0.16524792  0.13800709]]\n",
            "\n",
            "db2:\n",
            "[[-0.14033533]]\n",
            "\n",
            "dW1:\n",
            "[[ 0.1182918  -0.0591459 ]\n",
            " [-0.03768248  0.10496144]\n",
            " [-0.00349624 -0.17049228]\n",
            " [ 0.15772239 -0.0788612 ]]\n",
            "\n",
            "db1:\n",
            "[[ 0.0591459 ]\n",
            " [ 0.01403353]\n",
            " [-0.06749766]\n",
            " [ 0.0788612 ]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3.7.5: Backpropagation Algorithm**"
      ],
      "metadata": {
        "id": "LBS8aV90hDAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mathematical Intuition 3.7.11**\n",
        "\n",
        "The Backpropagation Algorithm involves the following steps to update the network's parameters and minimize the loss:\n",
        "\n",
        "1. Forward Propagation: Compute the linear combination $\\mathbf{Z}^{(l)}$ and activation $\\mathbf{A}^{(l)}$ for each layer.\n",
        "2. Compute Loss: Calculate the cost using the cost function (e.g., cross-entropy loss).\n",
        "3. Backward Propagation: Compute the gradients of the loss with respect to each parameter using the backpropagation process.\n",
        "5. Update Parameters: Adjust the weights and biases using the computed gradients and the learning rate.\n",
        "6. Repeat:Iterate through the forward and backward propagation steps for a predefined number of epochs or until convergence.\n"
      ],
      "metadata": {
        "id": "ENmix6vChfav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation Algorithm Implementation\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- Dictionary containing weights and biases.\n",
        "    grads -- Dictionary containing gradients.\n",
        "    learning_rate -- Learning rate for parameter updates.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- Dictionary with updated weights and biases.\n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "\n",
        "    for l in range(1, L + 1):\n",
        "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
        "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Example Usage of Backpropagation Algorithm\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize parameters\n",
        "    layer_dims = [2, 4, 1]\n",
        "    parameters = initialize_parameters(layer_dims)\n",
        "\n",
        "    # Generate dummy input data\n",
        "    X = np.array([[1, 2, -1],\n",
        "                  [3, -1, 2]])\n",
        "\n",
        "    # Define true labels\n",
        "    Y = np.array([[1, 0, 1]])\n",
        "\n",
        "    # Perform forward propagation\n",
        "    activations = forward_propagation(X, parameters)\n",
        "\n",
        "    # Compute cost\n",
        "    AL = activations[\"A2\"]\n",
        "    cost = cost_function(AL, Y)\n",
        "    print(f\"Initial Cost: {cost}\\n\")\n",
        "\n",
        "    # Perform backpropagation\n",
        "    grads = backward_propagation(parameters, activations, Y)\n",
        "\n",
        "    # Update parameters\n",
        "    learning_rate = 0.1\n",
        "    parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    # Perform forward propagation with updated parameters\n",
        "    activations = forward_propagation(X, parameters)\n",
        "    AL = activations[\"A2\"]\n",
        "    cost = cost_function(AL, Y)\n",
        "    print(f\"Updated Cost: {cost}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvQnMqbIiE3G",
        "outputId": "2aaaec97-4574-45ab-8d4a-3ea6c472099a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Cost: 6.057751944328228\n",
            "\n",
            "Updated Cost: 1.8257783625976673\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Analysis*\n",
        "1. Parameter Update (update_parameters):\n",
        " * Adjusts each weight and bias by subtracting the product of the learning rate and the corresponding gradient.\n",
        " * Ensures that the parameters move in the direction that minimizes the loss.\n",
        "\n",
        "2. Example Usage:\n",
        " * Initializes a simple neural network.\n",
        " * Generates dummy input data and defines true labels.\n",
        " * Performs forward propagation to compute activations.\n",
        " * Calculates the initial cost.\n",
        " * Executes backpropagation to compute gradients.\n",
        " * Updates the parameters using the gradients.\n",
        " * Performs forward propagation again to observe the change in cost"
      ],
      "metadata": {
        "id": "qOYSRpCTiRqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3.7.2: PyTorch Implementation**\n"
      ],
      "metadata": {
        "id": "5ChZ8gwzihl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Networks with PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)  # Input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(4, 1)  # Hidden layer to output layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleNN()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-oS1S6mjFwk",
        "outputId": "84b1a6f5-5f93-4e47-98fa-74563a67bac0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleNN(\n",
            "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
            "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Analysis*\n",
        "1. Model Architecture (SimpleNN):\n",
        " * fc1: Fully connected layer mapping 2 input features to 4 neurons in the hidden layer.\n",
        " * fc2: Fully connected layer mapping 4 neurons to 1 output neuron.\n",
        "2. Activation Functions:\n",
        " * ReLU: Applied after the first layer.\n",
        " * Sigmoid: Applied after the second layer to output probabilities.\n",
        "3. Model Instantiation:\n",
        " * Creates an instance of the SimpleNN class and prints its architecture."
      ],
      "metadata": {
        "id": "XmVNDcwQjOgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3.7.2: Training Example**\n"
      ],
      "metadata": {
        "id": "eBdHrkTujiE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Simple Neural Network with PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)  # Input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(4, 1)  # Hidden layer to output layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleNN()\n",
        "print(model)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
        "\n",
        "# Dummy data\n",
        "X = torch.tensor([[1.0, 0.0],\n",
        "                  [0.0, 1.0],\n",
        "                  [1.0, 1.0],\n",
        "                  [0.0, 0.0]])\n",
        "Y = torch.tensor([[1.0],\n",
        "                  [0.0],\n",
        "                  [1.0],\n",
        "                  [0.0]])\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X)\n",
        "    loss = criterion(outputs, Y)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Testing the model\n",
        "with torch.no_grad():\n",
        "    predicted = model(X)\n",
        "    predicted = (predicted >= 0.5).float()\n",
        "    print(\"Predicted labels:\\n\", predicted)\n",
        "    print(\"True labels:\\n\", Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fyBgtvzjmS-",
        "outputId": "f3d2e71a-e9e4-4821-e018-3d58cfa253e7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleNN(\n",
            "  (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
            "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Epoch [100/1000], Loss: 0.2867\n",
            "Epoch [200/1000], Loss: 0.0585\n",
            "Epoch [300/1000], Loss: 0.0247\n",
            "Epoch [400/1000], Loss: 0.0144\n",
            "Epoch [500/1000], Loss: 0.0098\n",
            "Epoch [600/1000], Loss: 0.0072\n",
            "Epoch [700/1000], Loss: 0.0057\n",
            "Epoch [800/1000], Loss: 0.0046\n",
            "Epoch [900/1000], Loss: 0.0039\n",
            "Epoch [1000/1000], Loss: 0.0034\n",
            "Training complete.\n",
            "Predicted labels:\n",
            " tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "True labels:\n",
            " tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Analysis*\n",
        "\n",
        "1. Model Definition:\n",
        " * Defines a simple neural network with one hidden layer using ReLU and sigmoid activation functions.\n",
        "2. Loss Function and Optimizer:\n",
        " * BCELoss: Suitable for binary classification tasks.\n",
        " * SGD: Optimizer with a learning rate of 0.1.\n",
        "3. Dummy Data:\n",
        " * Four samples with two features each, forming a simple XOR-like pattern.\n",
        "4. Training Loop:\n",
        " * Performs forward propagation, computes loss, backpropagates the gradients, and updates the model parameters.\n",
        " * Prints the loss every 100 epochs to monitor training progress.\n",
        "5. Testing the Model:\n",
        " * After training, makes predictions on the training data.\n",
        " * Applies a threshold of 0.5 to convert probabilities to binary labels.\n",
        " * Prints the predicted and true labels for comparison."
      ],
      "metadata": {
        "id": "p0ce3wcyjvdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3.7.3: Complete Neural Network**"
      ],
      "metadata": {
        "id": "brjxJpyikKIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Neural Network Training with Backpropagation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "def sigmoid(z):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    \"\"\"Derivative of sigmoid function.\"\"\"\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(z):\n",
        "    \"\"\"ReLU activation function.\"\"\"\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    \"\"\"Derivative of ReLU function.\"\"\"\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "# Initialize parameters\n",
        "def initialize_parameters(layer_dims):\n",
        "    \"\"\"\n",
        "    Initialize weights and biases.\n",
        "\n",
        "    Arguments:\n",
        "    layer_dims -- List containing the number of units in each layer.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- Dictionary containing initialized weights and biases.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)  # Number of layers\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Perform forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    X -- Input data, shape (n_x, m)\n",
        "    parameters -- Dictionary containing weights and biases.\n",
        "\n",
        "    Returns:\n",
        "    activations -- Dictionary containing activations for each layer.\n",
        "    Z_values -- Dictionary containing linear combinations for each layer.\n",
        "    \"\"\"\n",
        "    activations = {\"A0\": X}\n",
        "    Z_values = {}\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "\n",
        "    for l in range(1, L + 1):\n",
        "        W = parameters[f\"W{l}\"]\n",
        "        b = parameters[f\"b{l}\"]\n",
        "        Z = np.dot(W, activations[f\"A{l-1}\"]) + b\n",
        "        Z_values[f\"Z{l}\"] = Z\n",
        "\n",
        "        if l != L:\n",
        "            A = relu(Z)\n",
        "        else:\n",
        "            A = sigmoid(Z)\n",
        "        activations[f\"A{l}\"] = A\n",
        "\n",
        "    return activations, Z_values\n",
        "\n",
        "# Compute cost\n",
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy cost.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- Probability vector corresponding to label predictions, shape (1, m)\n",
        "    Y -- True \"label\" vector, shape (1, m)\n",
        "\n",
        "    Returns:\n",
        "    cost -- Cross-entropy cost\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "    cost = - (1/m) * np.sum(Y * np.log(AL + 1e-15) + (1 - Y) * np.log(1 - AL + 1e-15))\n",
        "    return np.squeeze(cost)\n",
        "\n",
        "# Backward propagation\n",
        "def backward_propagation(parameters, activations, Z_values, Y):\n",
        "    \"\"\"\n",
        "    Perform backward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- Dictionary containing weights and biases.\n",
        "    activations -- Dictionary containing activations from forward propagation.\n",
        "    Z_values -- Dictionary containing linear combinations for each layer.\n",
        "    Y -- True \"label\" vector, shape (1, m)\n",
        "\n",
        "    Returns:\n",
        "    grads -- Dictionary containing gradients with respect to each parameter.\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Initialize backpropagation\n",
        "    AL = activations[f\"A{L}\"]\n",
        "    dAL = - (np.divide(Y, AL + 1e-15) - np.divide(1 - Y, 1 - AL + 1e-15))\n",
        "\n",
        "    # Backprop for output layer\n",
        "    dZL = AL - Y  # derivative of cost w.r.t Z at output layer\n",
        "    grads[f\"dW{L}\"] = (1/m) * np.dot(dZL, activations[f\"A{L-1}\"].T)\n",
        "    grads[f\"db{L}\"] = (1/m) * np.sum(dZL, axis=1, keepdims=True)\n",
        "\n",
        "    # Backprop through hidden layers\n",
        "    for l in reversed(range(1, L)):\n",
        "        dA = np.dot(parameters[f\"W{l+1}\"].T, dZL)\n",
        "        dZ = dA * relu_derivative(Z_values[f\"Z{l}\"])\n",
        "        grads[f\"dW{l}\"] = (1/m) * np.dot(dZ, activations[f\"A{l-1}\"].T)\n",
        "        grads[f\"db{l}\"] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "        dZL = dZ  # Update dZL for next iteration\n",
        "\n",
        "    return grads\n",
        "\n",
        "# Update parameters\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- Dictionary containing weights and biases.\n",
        "    grads -- Dictionary containing gradients.\n",
        "    learning_rate -- Learning rate for parameter updates.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- Dictionary with updated weights and biases.\n",
        "    \"\"\"\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "\n",
        "    for l in range(1, L + 1):\n",
        "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
        "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Neural network model\n",
        "def neural_network_model(X, Y, layer_dims, learning_rate=0.01, num_iterations=10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Train a neural network.\n",
        "\n",
        "    Arguments:\n",
        "    X -- Input data, shape (n_x, m)\n",
        "    Y -- True labels, shape (1, m)\n",
        "    layer_dims -- List containing the dimensions of each layer.\n",
        "    learning_rate -- Learning rate for gradient descent.\n",
        "    num_iterations -- Number of iterations to train.\n",
        "    print_cost -- If True, print the cost every 1000 iterations.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- Trained weights and biases.\n",
        "    \"\"\"\n",
        "    parameters = initialize_parameters(layer_dims)\n",
        "\n",
        "    for i in range(1, num_iterations + 1):\n",
        "        # Forward propagation\n",
        "        activations, Z_values = forward_propagation(X, parameters)\n",
        "\n",
        "        # Compute cost\n",
        "        cost = compute_cost(activations[f\"A{len(layer_dims)-1}\"], Y)\n",
        "\n",
        "        # Backward propagation\n",
        "        grads = backward_propagation(parameters, activations, Z_values, Y)\n",
        "\n",
        "        # Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # Print cost\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print(f\"Cost after iteration {i}: {cost:.6f}\")\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Define layer dimensions\n",
        "    layer_dims = [2, 4, 1]  # 2 input features, 4 neurons in hidden layer, 1 output neuron\n",
        "\n",
        "    # Generate dummy input data (2 features, 3 examples)\n",
        "    X = np.array([[1, 2, -1],\n",
        "                  [3, -1, 2]])  # Shape: (2, 3)\n",
        "\n",
        "    # Define true labels\n",
        "    Y = np.array([[1, 0, 1]])  # Shape: (1, 3)\n",
        "\n",
        "    # Train the neural network\n",
        "    parameters = neural_network_model(X, Y, layer_dims, learning_rate=0.1, num_iterations=10000, print_cost=True)\n",
        "\n",
        "    # Perform forward propagation with trained parameters\n",
        "    activations, Z_values = forward_propagation(X, parameters)\n",
        "\n",
        "    # Compute final cost\n",
        "    final_cost = compute_cost(activations[f\"A{len(layer_dims)-1}\"], Y)\n",
        "    print(f\"\\nFinal Cost: {final_cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAsu7a27kKef",
        "outputId": "0e504ba8-f119-4b99-df11-c786230f3cfc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 1000: 0.000804\n",
            "Cost after iteration 2000: 0.000328\n",
            "Cost after iteration 3000: 0.000199\n",
            "Cost after iteration 4000: 0.000141\n",
            "Cost after iteration 5000: 0.000108\n",
            "Cost after iteration 6000: 0.000088\n",
            "Cost after iteration 7000: 0.000073\n",
            "Cost after iteration 8000: 0.000063\n",
            "Cost after iteration 9000: 0.000055\n",
            "Cost after iteration 10000: 0.000049\n",
            "\n",
            "Final Cost: 4.853236898537035e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Code Analysis*\n",
        "1. Activation Functions and Derivatives:\n",
        " * Sigmoid: Used in the output layer for binary classification.\n",
        " * ReLU: Used in hidden layers for non-linear transformations.\n",
        "\n",
        "2. Parameter Initialization (initialize_parameters):\n",
        " * Initializes weights with small random values and biases with zeros based on layer dimensions.\n",
        "3. Forward Propagation (forward_propagation):\n",
        " * Computes activations and linear combinations for each layer.\n",
        " * Uses ReLU for hidden layers and sigmoid for the output layer.\n",
        "4. Cost Calculation (compute_cost):\n",
        " * Computes cross-entropy loss between predicted probabilities and true labels.\n",
        "5. Backward Propagation (backward_propagation):\n",
        " * Computes gradients of the loss with respect to weights and biases using the chain rule.\n",
        " * Applies derivatives of activation functions.\n",
        "6. Parameter Update (update_parameters):\n",
        " * Updates weights and biases using gradient descent.\n",
        "7. Neural Network Model (neural_network_model):\n",
        " * Trains the neural network over a specified number of iterations.\n",
        " * Optionally prints the cost at intervals to monitor training.\n",
        "8. Example Usage:\n",
        " * Trains a simple neural network on dummy data.\n",
        " * Prints the final cost after training."
      ],
      "metadata": {
        "id": "9vnAXxjAkdLs"
      }
    }
  ]
}